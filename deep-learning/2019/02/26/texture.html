<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Texture Classification with CNN | Hao Chun Chang’s Blog</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Texture Classification with CNN" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This note is based on a great survey" />
<meta property="og:description" content="This note is based on a great survey" />
<link rel="canonical" href="https://haochunchang.github.io/deep-learning/2019/02/26/texture.html" />
<meta property="og:url" content="https://haochunchang.github.io/deep-learning/2019/02/26/texture.html" />
<meta property="og:site_name" content="Hao Chun Chang’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-26T00:00:00+00:00" />
<script type="application/ld+json">
{"description":"This note is based on a great survey","mainEntityOfPage":{"@type":"WebPage","@id":"https://haochunchang.github.io/deep-learning/2019/02/26/texture.html"},"url":"https://haochunchang.github.io/deep-learning/2019/02/26/texture.html","@type":"BlogPosting","headline":"Texture Classification with CNN","dateModified":"2019-02-26T00:00:00+00:00","datePublished":"2019-02-26T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">

  <!-- favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/images/favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/images/favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/images/favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/images/favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/images/favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/images/favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/images/favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/images/favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/images/favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/images/favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="/assets/images/favicon/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/images/favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <!-- for mathjax support -->
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link type="application/atom+xml" rel="alternate" href="https://haochunchang.github.io/feed.xml" title="Hao Chun Chang's Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Hao Chun Chang&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/daily_posts.html">Daily</a><a class="page-link" href="/blog/deep_learning_posts.html">Deep-Learning</a><a class="page-link" href="/blog/programming_posts.html">Programming</a><a class="page-link" href="/blog/tools_posts.html">Tools</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Texture Classification with CNN</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-02-26T00:00:00+00:00" itemprop="datePublished">Feb 26, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="this-note-is-based-on-a-great-survey">This note is based on <a href="https://arxiv.org/pdf/1801.10324v2.pdf">a great survey</a></h2>

<ul>
  <li>Texture Representation:
    <ul>
      <li><a href="#Bag-of-Word-based-methods-(BoW)">BoW-based representation</a></li>
      <li><a href="#Convolutional-Neural-Network-based-methods">CNN-based representation</a></li>
      <li><a href="#Attribute-based-methods">Attribute-based representation</a></li>
    </ul>
  </li>
  <li><a href="#Texture-Datasets">Texture Datasets</a></li>
</ul>

<hr />

<h2 id="texture-representation">Texture Representation</h2>
<ul>
  <li>Contrast to object classification, the global spatial information is less important.</li>
</ul>

<h3 id="bag-of-word-based-methods-bow">Bag-of-Word based methods (BoW)</h3>

<p><img src="/assets/images/texture-clf/BoW-pipeline.png" alt="BoW pipeline" /></p>

<ol>
  <li>Local Patch Extraction</li>
  <li>Local Patch Representation (Feature Descriptors)
    <ul>
      <li>Ideal: distinctive, robust to variations</li>
    </ul>
  </li>
  <li>Codebook Generation
    <ul>
      <li>Find a set of prototype features from training data.</li>
      <li>Like words, phrases in languages. (Similar to dimension reduction)</li>
    </ul>
  </li>
  <li>Feature Encoding
    <ul>
      <li>Assign local representation to some prototype features.</li>
    </ul>
  </li>
  <li>Feature Pooling</li>
  <li>Feature Classification</li>
</ol>

<h3 id="convolutional-neural-network-based-methods">Convolutional Neural Network based methods</h3>

<ul>
  <li>The survey classified the methods into 3 categories:
    <ol>
      <li><a href="#Pre-trained-Generic-CNN-models">Using pre-trained generic CNN models</a></li>
      <li><a href="#Fine-tuned-CNN-models">Using fine-tuned CNN models</a></li>
      <li><a href="#Texture-specific-CNN-models">Using texture-specific CNN models</a></li>
    </ol>
  </li>
</ul>

<h5 id="pre-trained-generic-cnn-models">Pre-trained Generic CNN models</h5>

<ul>
  <li>CNN = convolution + non-linear activation + pooling
    <ul>
      <li>Related to LBP, Random Projection (RP), etc.</li>
      <li>CNN-extracted features can be encoded by BoW-based method.</li>
    </ul>
  </li>
</ul>

<h5 id="fine-tuned-cnn-models">Fine-tuned CNN models</h5>

<p><img src="/assets/images/texture-clf/Finetune-CNN.png" alt="Fine-tuned CNN" /></p>

<ul>
  <li><a href="https://arxiv.org/pdf/1601.02919.pdf">Texture CNN (T-CNN)</a>
    <ul>
      <li>Energy Layer: average activation ouput for each feature map of the last conv layers.
        <ul>
          <li>1 value per feature map</li>
          <li>Similar to energy response of a filter bank.</li>
          <li>Example: 256x27x27 (channel x height x width) \(\rightarrow\) 256x1</li>
        </ul>
      </li>
      <li>Concat: GlobalAveragePooling(intermediate conv.) and last conv.</li>
      <li>Insight:
        <ul>
          <li>Fine-tune a texture-centric pretrained network performs better than that pretrained with object-centric dataset.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/1504.07889.pdf">Bilinear CNN (BCNN)</a>
    <ul>
      <li>Replace FC layers with <strong>orderless bilinear pooling</strong> layer. (Matrix outer product + aveage pooling)
        <ul>
          <li>\(f(l, I)\): Feature function, l = locations, I = image</li>
          <li>
\[f: L \times I \rightarrow R^{K\times D}\]
          </li>
        </ul>
      </li>
      <li>Cost: High dimenstional features \(\rightarrow\) need lots of training data</li>
    </ul>
  </li>
</ul>

\[bilinear(l, I, f_A, f_B) = f_A(l, I)^{T}f_B(l, I)\]

<ul>
  <li><a href="https://arxiv.org/pdf/1511.06062.pdf">Compact BCNN</a>
    <ul>
      <li>To reduce the BCNN feature dimensions: Approximate the inner product of 2 bilinear features.
        <ul>
          <li>Random Maclaurin Projection or Tensor Sketch Projection</li>
        </ul>
      </li>
      <li>To reduce time, represent the bilinear features as a matrix and applied a low rank bilinear classifier
        <ul>
          <li><a href="https://arxiv.org/pdf/1611.05109.pdf">Ref</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_FASON_First_and_CVPR_2017_paper.pdf">FASON (First And Second Order information fusion Network)</a>
    <ul>
      <li>T-CNN + Compact BCNN</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/1511.07247.pdf">NetVLAD</a>
    <ul>
      <li>VLAD = Vector of Locally Aggregated Descriptors</li>
    </ul>
  </li>
</ul>

\[V(j, k) = \sum{N}{i=1}a_k(x_i)(x_i(j) - c_k(j))\]

<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1612.02844.pdf">Deep Texture Encoding Network (DeepTEN)</a></p>
  </li>
  <li>Residual encoding layer:
    <ul>
      <li>Each input \(x_i\) is assigned a weight to the codewords.</li>
      <li>Contrast to soft-weight assignment, the smoothing factor for each cluster is learnable</li>
    </ul>
  </li>
  <li>Residual encoding: (Output = \(e_k\) for each codeword \(c_k\)):</li>
</ul>

\[r_{ik} = x_i - c_k\]

\[e_k = \sum{N}{i=1}e_{ik} = \sum{N}{i=1}a_{ik}r_{ik}\]

<ul>
  <li>Soft-weight assignment (\(a_{ik}\))
    <ul>
      <li>\(\beta\) is the smoothing factor</li>
      <li>\(\beta \rightarrow s_k\) where \(s_k\) is learnable</li>
    </ul>
  </li>
</ul>

\[a_{ik} = \frac{exp(-\beta \|r_{ik}\|^2)}{\sum{K}{j=1}exp(-\beta \|r_{ik}\|^2)}\]

<h5 id="texture-specific-cnn-models">Texture-specific CNN models</h5>

<p><img src="/assets/images/texture-clf/Texture-specific.png" alt="Texture-specific CNN" /></p>

<ul>
  <li><a href="https://www.di.ens.fr/~mallat/papiers/Bruna-Mallat-Pami-Scat.pdf">ScatNet</a>
    <ul>
      <li>Pre-determined convolution layers (Example: Haar, Gabor wavelets)
        <ul>
          <li>Translation-invariant, also extends to rotation and scale invariance.</li>
          <li>No need to learn, but expensive when extracting features.</li>
        </ul>
      </li>
      <li>Explore theoretical aspect of CNN.</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/pdf/1404.3606.pdf">PCANet</a>
    <ul>
      <li>Use trained PCA filters.</li>
      <li>Variations: RandNet, LDANet</li>
      <li>Faster feature extraction than ScatNet. Weaker invariance and performance.</li>
    </ul>
  </li>
</ul>

<h3 id="attribute-based-methods">Attribute based methods</h3>
<ul>
  <li>Allows more detail description of an image:
    <ul>
      <li>Like: spotted, striated, striped, etc.</li>
    </ul>
  </li>
  <li>Issues:
    <ul>
      <li>Need a unified vocabulary for describing texture attribure.</li>
      <li>Benchmark dataset annotated by semantic attriburte.</li>
    </ul>
  </li>
  <li>Studies that characterized texture attribute:
    <ol>
      <li><a href="">Tamura <em>et al.</em></a>: coarseness, contrast, directionality, line-likeness, regularity and roughness</li>
      <li><a href="">Amadasun and King</a>: coarseness, contrast, business, complexity, and strength (refine 1.)</li>
      <li><a href="">Matthews <em>et al.</em></a>: 11 commonly-used attributes by using a single adjective. (Relative comparison)</li>
    </ol>
  </li>
</ul>

<hr />

<h2 id="texture-datasets">Texture Datasets</h2>

<p><img src="/assets/images/texture-clf/Dataset.png" alt="Summary" /></p>

  </div><a class="u-url" href="/deep-learning/2019/02/26/texture.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Hao Chun Chang&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p>This is a blog post about a software engineer started from biochemistry to medical artifical intelligence.</p>
      </div>

      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">
            Contact Me
          </li><li><a class="u-email" href="mailto:changhaochun84@gmail.com">changhaochun84@gmail.com</a></li></ul>
      </div>


      <div class="footer-col footer-col-3"><ul class="social-media-list"><li><a href="https://github.com/haochunchang"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">haochunchang</span></a></li><li><a href="https://www.linkedin.com/in/haochunchang84"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">haochunchang84</span></a></li></ul>
</div>

    </div>

  </div>

</footer>
</body>

</html>
